---

title: 'Develop agents in TypeScript'

description: 'Use the Blaxel SDK to develop and run a custom agent in TypeScript.'

---

You can **bring agents developed in TypeScript in any framework** (Vervel, LlamaIndex, LangChain, VercelAI, or any custom framework) and deploy them on Blaxel by integrating a few lines of the Blaxel SDK and leveraging our other developer tools ([Blaxel CLI](../cli-reference), GitHub action, etc.).

<Tip>Check out [this Getting Started](../Get-started) tutorial in order to develop and deploy your first Hello World AI agent globally in less than 5 minutes.</Tip>

## How to start ?

If you already know the SDK, you could directly insert our SDK in your project, and deploy it as it is.  
But if it's your first time, we recommend you to follow the steps below.

### Step 1: Create a new project

This command will create a new project from a template, allowing to choose the framework you want to use.
```sh
bl create-agent-app my-tool-agent

┃ Project Name                                                                  
┃ Name of your agent app                                                        
┃ > my-tool-agent                                                               
                                                                                
  Language                                                                      
  Language to use for your agent app                                            
  > typescript                                                                  
                                                                                
                                                                                
                                                                                
  Template                                                                      
  Template to use for your agent app                                            
  > vercel-ai                                                                   
    llamaindex                                                                  
    langchain   
```

### Step 2: Run the agent locally

You can start your agent from the folder generated by the command above.

```bash
cd my-agent
bl serve --hotreload
```

Then you can directly interact with your agent by calling the standard endpoint already generated.

```bash
bl run agent my-agent --local --data '{"inputs": "Hello, how are you?"}'

// OR with a chat interface

bl chat my-agent --local
```

### Step 3: Implement your agent

In the folder generated you will retrieve a standard server in the entrypoint file `index.ts`.  
In most case you will not have to work here, but if you need to add specific logic, you can do it.

The main place to work will be the `agent.ts` file.  
Depending of the framework you have chosen, the code will be different.  
Our framework is made to give you ability to use the ability of Blaxel platform without transforming the main logic of your agent.

The generated template file is a simple agent, using a Blaxel platform tool (MCP server) to search on the web, and also a local tool to show you how implement one locally.

If you develop a bunch of tools, we recommend you to implement a MCP server with the tutorial [here](../Functions/Create-mcp-server).

<CodeGroup>
```ts agent.ts (Vercel AI)
import { blModel, blTools, logger } from '@blaxel/sdk';
import { streamText, tool } from 'ai';
import { z } from 'zod';

interface Stream {
  write: (data: string) => void;
  end: () => void;
}

export default async function agent(input: string, stream: Stream): Promise<void> {
  const response = streamText({
    experimental_telemetry: { isEnabled: true },
    // dynamic load of model from blaxel platform
    model: await blModel("gpt-4o-mini").ToVercelAI(),
    tools: {
      // dynamic load of tools from blaxel platform
      ...await blTools(['blaxel-search']).ToVercelAI(),
      // And here an example of a local tool for vercel ai framework
      "weather": tool({
        description: "Get the weather in a specific city",
        parameters: z.object({
          city: z.string(),
        }),
        execute: async (args: { city: string }) => {
          logger.debug("TOOLCALLING: local weather", args);
          return `The weather in ${args.city} is sunny`;
        },
      }),
    },
    system: "You are an agent which will give the weather when a city is provided, and also do a quick search about this city.",
    messages: [
      { role: 'user', content: input }
    ],
    maxSteps: 5,
  });

  for await (const delta of response.textStream) {
    stream.write(delta);
  }
  stream.end();
}
```

```typescript agent.ts (LlamaIndex)
import { blModel, blTools, logger } from '@blaxel/sdk';
import { agent, AgentStream, tool, ToolCallLLM } from "llamaindex";
import { z } from "zod";
interface Stream {
  write: (data: string) => void;
  end: () => void;
}

export default async function myagent(input: string, stream: Stream): Promise<void> {
  const streamResponse = agent({
    llm: await blModel("gpt-4o-mini").ToLlamaIndex() as unknown as ToolCallLLM,
    tools: [...await blTools(['blaxel-search']).ToLlamaIndex(),
      tool({
        name: "weather",
        description: "Get the weather in a specific city",
        parameters: z.object({
          city: z.string(),
        }),
        execute: async (input) => {
          logger.debug("TOOLCALLING: local weather", input)
          return `The weather in ${input.city} is sunny`;
        },
      })
    ],
    systemPrompt: "If the user ask for the weather, use the weather tool.",
  }).run(input);

  for await (const event of streamResponse) {
    if (event instanceof AgentStream) {
      for (const chunk of event.data.delta) {
        stream.write(chunk);
      }
    }
  }
  stream.end();
}
```

```typescript agent.ts (LangChain)
import { blModel, blTools, logger } from '@blaxel/sdk';
import { HumanMessage } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { z } from "zod";
interface Stream {
  write: (data: string) => void;
  end: () => void;
}

export default async function agent(input: string, stream: Stream): Promise<void> {
  const streamResponse = await createReactAgent({
    llm: await blModel("gpt-4o-mini").ToLangChain(),
    prompt: "If the user ask for the weather, use the weather tool.",
    tools: [
      ...await blTools(['blaxel-search']).ToLangChain(),
      tool(async (input: any) => {
        logger.debug("TOOLCALLING: local weather", input)
        return `The weather in ${input.city} is sunny`;
      },{
        name: "weather",
        description: "Get the weather in a specific city",
        schema: z.object({
          city: z.string(),
        })
      })
    ],
  }).stream({
    messages: [new HumanMessage(input)],
  });

  for await (const chunk of streamResponse) {
    if(chunk.agent) for(const message of chunk.agent.messages) {
      stream.write(message.content)
    }
  }
  stream.end();
}
```
</CodeGroup>



## Blaxel agent template file structure

### Overview

```tree
package.json            # This file is the standard package.json file, mandatory, it define the entrypoint of the project, and dependencies.
blaxel.toml             # This file is a project configuration dedicated to Blaxel platform, it's not mandatory, but it allow you to customize the deployment.
tsconfig.json           # This file is the standard tsconfig.json file, only needed if you use typescript, like in our templates.
.blaxel                 # this folder allow you to add custom resources, they will be deployed with your agent.
├── blaxel-search.yaml  # Here blaxel-search is a standard Web search tool, you can use it to develop your first agent, but the rate limit is low, we recommend to use a MCP server deploy by yourself on the platform for production.
src/
└── index.ts            # This file is the standard entrypoint of the project, it's used to start the server, and create an endpoint bind with agent.ts file.
├── agent.ts            # This file is the main file of your agent, it's loaded from index.ts, in the template, all the agent logic is implemented here.
```


### package.json

Here the main import things are the scripts, they are use for the `bl serve` and `bl deploy` commands.

```json
{
  "name": "vercel-first",
  "version": "1.0.0",
  "description": "<no value>",
  "keywords": [],
  "license": "MIT",
  "author": "cdrappier",
  "scripts": {
    "start": "tsx src/index.ts",
    "prod": "node dist/index.js",
    "dev": "tsx watch src/index.ts",
    "build": "tsc"
  },
  "dependencies": {
    "@ai-sdk/openai": "^1.2.5",
    "@blaxel/sdk": "0.1.1-preview.9",
    "ai": "^4.1.61",
    "fastify": "^5.2.1",
    "zod": "^3.24.2"
  },
  "devDependencies": {
    "@types/express": "^5.0.1",
    "@types/node": "^22.13.11",
    "tsx": "^4.19.3",
    "typescript": "^5.8.2"
  }
}

Depending of what you do, all of them are not required.
With typescript, we use the 4 of them.

`start` : start the server locally, directly through the typescript command, to not have to build the project when developing.
`build` : build the project, it's done automatically when deploying.
`prod` : start the server remotely from the dist folder, build need to be done before.
`dev` : same as start, but with hotreload, it's useful when developing locally, each file change is reflected immediately.

Other fields of package.json are standard to javascript/typescript projects, you can add your own dependencies as you want.
Just take in mind than devDependencies will be use only for the build command, we remove them right after.

### blaxel.toml

This file is used to configure the deployment of the agent on Blaxel platform.
It's not mandatory, but it allow you to customize the deployment.

```toml
name = "my-agent"
workspace = "my-workspace"
type = "agent"

agents = []
functions = ["blaxel-search"]
models = ["gpt-4o-mini"]

[env]
DEFAULT_CITY = "San Francisco"
```

Here `name`, `workspace`, `type` are not mandatory, but each bl command in the folder, will use them instead of asking you to provide them.

`agents`, `functions`, `models` are used to specify the resources that will be deployed with the agent, not mandatory, but if you specify them here, it will be preload with your build, removing any dependency to our controlplane during runtime, so your runtime will be faster!

`env` is used to specify the environment variables that will be used in the agent, it's not secret, but it allow you to retrieve them from the code easily from our SDK.


<Card title="Deploy an agent" icon="server" href="/Agents/Deploy-an-agent">
Learn how to deploy your custom AI agents on Blaxel as a serverless endpoint. 
</Card>