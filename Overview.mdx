---

title: 'Beamlit Documentation'

description: 'Welcome to Beamlit!'

sidebarTitle: "Overview"

---

Beamlit is a **global infrastructure** that gives ML Platform Engineers the tools to deploy and run AI models and agents across multiple regions, ensuring **very low-latency and very high-availability**. 

This portal provides comprehensive documentation, API references, and CLI instructions to help you operate the platform effectively.

## Core concepts

Beamlit's architecture is both **serverless** and **distributed**, automatically scaling compute resources without requiring server management. 

At its core, Beamlit's **Global Inference Network** serves as the backbone, designed to deliver inferences at scale with exceptional availability and minimal latency. This allows for multi-region deployment, enabling AI workloads to run across multiple geographic areas or cloud providers. 

For organizations with on-premise setups, Beamlit offers the **Cloud Burst Network** feature. This allows for minimal-footprint deployments with overflow capacity, providing flexibility in resource management. Additionally, Beamlit provides real-time observability, allowing users to monitor and analyze performance metrics for their model deployments.

Among the important concepts in Beamlit are **models** and **deployments**. Models serve as the foundational logical entities, while deployments represent specific versions of these models implemented in various environments. Environments like development and production function as logical constructs for overseeing the model lifecycle across different stages. Policies play a crucial role in this framework, establishing and enforcing guidelines that govern the deployment process, thus ensuring adherence to organizational standards.

The platform implements advanced security measures, including fine-grained authentication and authorization through Beamlit IAM, ensuring that your AI infrastructure remains protected.

The platform supports model deployment and management through various methods, including APIs, CLI, console, or Kubernetes operator.

## Documentation structure

Our documentation is organized into the following main sections:

- [**Get started**](Get-started.mdx): Deploy your first AI model worldwide in just 3 minutes.
- [**Examples:**](Examples/Deploy-a-custom-model-from-HuggingFace.mdx) Explore real-world use cases and implementation guides.
- **Product Documentation**
    - [**Models:**](Models/Model-deployment.mdx) Learn about supported model types and deployment strategies.
    - [**Model Governance:**](Model-Governance/Environments.mdx) Understand best practices for managing your models.
    - [**Observability:**](Observability/View-model-logs.mdx) Monitor and optimize your model performance.
    - [**Security:**](Security/Workspace-access-control.mdx) Implement robust security measures for your ML infrastructure.
    - [**Integrations:**](Integrations/HuggingFace.mdx) Discover how Beamlit works with other tools and platforms.
- [**API reference:**](https://docs.beamlit.com/api-reference/introduction) Comprehensive guide to Beamlit's APIs.
- [**CLI reference:**](https://docs.beamlit.com/cli-reference/introduction) Learn how to use Beamlit's command-line interface.