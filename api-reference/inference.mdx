---

title: 'Inference API'

description: 'Run inferences on your Beamlit deployments.'

---

Whenever you deploy a workload on Beamlit, an **inference endpoint** is generated on Global Inference Network, the infrastructure powerhouse that hosts it.

The inference API URL depends on the type of workload ([agent](../Agents/Overview%2015347e47b1ea807bbba6d269c01e633d), [model API](../Models/Overview%2015347e47b1ea80f48cbcd345aafd924a), [function](../Functions/Overview)) you are trying to request:

<CodeGroup>

```http Query agent

POST run.beamlit.com/{your-workspace}/agents/{your-agent}

```

```http Query model API

POST run.beamlit.com/{your-workspace}/models/{your-model}

```

```http Query function

POST run.beamlit.com/{your-workspace}/functions/{your-function}

```

</CodeGroup>

Showing the full request, with the input payload:

<CodeGroup>

```http Query agent

curl -X POST "https://run.beamlit.com/{your-workspace}/agents/{your-agent}" \
-H 'Content-Type: application/json' \
-H "X-Beamlit-Authorization: Bearer <YOUR_API_KEY>" \
-d '{"inputs":"Hello, world!"}'

```

```http Query model API (example on a chat completion model)
curl -X POST "run.beamlit.com/{your-workspace}/models/{your-model}/chat/completions" \
-H 'Content-Type: application/json' \
-H "X-Beamlit-Authorization: Bearer <YOUR_API_KEY>" \
-d '{"messages":[{"role":"user","content":"Hello!"}]}'

```

```http Query function (pre-built functions)
curl -X POST "run.beamlit.com/{your-workspace}/functions/{your-function}/tools/call" \
-H 'Content-Type: application/json' \
-H "X-Beamlit-Authorization: Bearer <YOUR_API_KEY>" \
-d '{"name":"brave_web_search","arguments":{"query":"current weather in San Francisco"}}'

```

```http Query function (custom functions)
curl -X POST "run.beamlit.com/{your-workspace}/functions/{your-function}/tools/call" \
-H 'Content-Type: application/json' \
-H "X-Beamlit-Authorization: Bearer <YOUR_API_KEY>" \
-d $'{"inputs":"Enter your input here."}'

```

</CodeGroup>

There is one distinct endpoint for each **deployment,** i.e. for each combination of an object and an [environment](../Model-Governance/Environments) in which it is deployed. 

For example, if you have one version of model API “your-model” deployed on the *production* environment and one version deployed on the *development* environment:

- `run.beamlit.com/{your-workspace}/models/{your-model}?environment=production` will call the production deployment

- `run.beamlit.com/{your-workspace}/models/{your-model}?environment=development` will call the development deployment

If you do not specify the environment in the inference request, it will call the *production* environment by default. If the model API is not deployed on the production environment, it will return an error.

<Card title="Product documentation" icon="earth-americas" href="/Models/Query-agents" > Read our product guide on querying an agent. </Card>